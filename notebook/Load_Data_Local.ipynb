{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "008ad845",
   "metadata": {},
   "source": [
    "# Load_data/metadata generator\n",
    "Written by Fernanda Fossa @fefossa\n",
    "\n",
    "Python 3.9.12\n",
    "\n",
    "**Description**: This notebook generates a CSV file to be used by CellProfiler module LoadData, using a list of the images to be analyzed. At the end, it will also generate a **AWS command** to upload the CSV files and images to AWS S3 bucket using AWS cli, based on the user inputs. These files together with CellProfiler pipelines will be processed within AWS using Distributed-CellProfiler. See more at https://github.com/DistributedScience/Distributed-CellProfiler\n",
    "\n",
    "**Inputs**: it requires the following inputs:\n",
    "\n",
    "- Path to the input folder (where images are located) = Make sure to have only the images that will be analyzed in this folder.\n",
    "\n",
    "- project_name = this is the name of the folder in AWS where images and CSV will be uploaded.\n",
    "\n",
    "- batch_id = this is the name of each subproject inside project_name folder. You can have different subprojects within one project. \n",
    "\n",
    "- Channels dictionary = Dictionary with Channel as a key and the name you want to give as a Value. Example: 'DAPI':'OrigDNA'. DAPI is the channel (written in the image name), and OrigDNA is the name we want to give to this image in CellProfiler. We have three options already available to choose from (Cell Painting, Live Cell Painting, ToxPath panels); if yours is different, please provide a new dictionary. \n",
    "\n",
    "**Outputs**: \n",
    "\n",
    "1. **load_data.csv** that we use with Illumination Correction pipelines. \n",
    "\n",
    "    To extract these informations, we are using a regex adapted to files from Cytation 5 microscope (B10_02_1_10_GFP_001.tif), where the location of the Well, Site and Channel is known. If you have images from different microscope, with a different pattern, you'd need to **change the regex**. \n",
    "\n",
    "    - FileName_CHANNEL = CHANNEL as the value you provided in the dictionary. It will extract the names of the images from the input folder. \n",
    "\n",
    "    - PathName_CHANNEL = containing a specific path to AWS (which will be used later in the virtual machines). To change that you would need to modify the images_dir variable.\n",
    "\n",
    "    - Metadata_Well and Metadata_Site = both are extracted from the image filename. \n",
    "\n",
    "    - Metadata_Plate = usually the name of the plate is the name of the FOLDER where the images are located with. Notice that we also have a regex for the plate name. We replace any spaces with \"_\" because AWS does not handle spaces well.\n",
    "\n",
    "2. **load_data_with_illum.csv** that we use for analysis pipelines after Illum Correction was performed. \n",
    "\n",
    "    The columns are the same as above, with two additional columns per channel:\n",
    "\n",
    "    - FileName_IllumCHANNEL = the name of the Illum Correction file (the name pattern is \"PlateName_IllumCHANNEL.npy\")\n",
    "\n",
    "    - PathName_IllumCHANNEL = the path in AWS where these Illum files will be located. Again this is a pattern for who's using AWS with Distributed-CellProfiler.\n",
    "\n",
    "3. **AWS commands**: based on the project_name, batch_id, and plate, we generate two commands that you can paste in the Command prompt (after installing and setup https://aws.amazon.com/cli/) that will upload the images and CSV files into AWS S3 bucket using aws s3 sync command. \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2196b8a",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1796517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import walk\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import easygui as eg\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "import pyperclip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720f10b2",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cdd17b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = eg.enterbox('Paste path to input folder and press OK', 'Paste Path')\n",
    "# input_folder = input(r\"Insert your local path to the folder images here:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d632235",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = eg.enterbox('Write project_name here (folder name)', 'Project_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2fe83bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_id = eg.enterbox('Write batch_id here (subproject name, second folder)', 'batch_id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6199b5ca",
   "metadata": {},
   "source": [
    "## Dictionary with channel as a key and new name as a value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77b679c9",
   "metadata": {},
   "source": [
    "### Create your own dictionary\n",
    "\n",
    "- Enter first the name of the Channel, and then the new name. When you finish, just write **done** and press ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bb9a59d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DAPI': 'OrigDNA', 'GFP': 'Acridine'}\n"
     ]
    }
   ],
   "source": [
    "ch_dic = {}\n",
    "\n",
    "while True:\n",
    "    channel = eg.enterbox('Enter CHANNEL name (e.g. DAPI). Enter done to finish input ')\n",
    "    if channel == 'done':\n",
    "        break\n",
    "    else:\n",
    "        name = eg.enterbox('Enter the NEW NAME (e.g. OrigDNA): ')\n",
    "        ch_dic[channel] = name\n",
    "\n",
    "print(ch_dic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fc93868",
   "metadata": {},
   "source": [
    "### OR Run one of the cells below to use our pre-made dictionaries\n",
    "\n",
    "- We have dictionaries for Cell Painting, Live Cell Painting, and ToxPath image panels names. \n",
    "\n",
    "- Run **ONLY ONE OF THE CELLS BELOW**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b118cca",
   "metadata": {},
   "source": [
    "#### For CELL PAINTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4a52e367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DAPI': 'OrigDNA', 'GFP': 'OrigER', 'PropidiumIodide': 'OrigAGP', 'CY5': 'OrigMito'}\n"
     ]
    }
   ],
   "source": [
    "ch_dic = {'DAPI':'OrigDNA', 'GFP':'OrigER', 'PropidiumIodide':'OrigAGP', 'CY5':'OrigMito'}\n",
    "print(ch_dic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2507779f",
   "metadata": {},
   "source": [
    "#### For LIVE CELL PAINTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1062fea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GFP': 'AOGFP', 'PropidiumIodide': 'AOPI'}\n"
     ]
    }
   ],
   "source": [
    "ch_dic = {'GFP':'AOGFP', 'PropidiumIodide':'AOPI'}\n",
    "print(ch_dic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5168c67a",
   "metadata": {},
   "source": [
    "#### For TOXPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4b779bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DAPI': 'OrigDNA', 'GFP': 'OrigLipids', 'TexasRed': 'OrigH2ax', 'CY5': 'OrigNfkb'}\n"
     ]
    }
   ],
   "source": [
    "ch_dic = {'DAPI':'OrigDNA', 'GFP':'OrigLipids', 'TexasRed':'OrigH2ax', 'CY5':'OrigNfkb'}\n",
    "print(ch_dic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b418840e",
   "metadata": {},
   "source": [
    "## Make sure the inputs are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc903196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input folder: G:\\My Drive\\Training\\20230130_TrainingNanoCell\\images\\211015_065907_Plate 1\n",
      "Project name: Training_NanoCell\n",
      "Batch id: 2021_10_08_AgNPViability\n",
      "Channels dictionary: {'GFP': 'AOGFP', 'PropidiumIodide': 'AOPI'}\n"
     ]
    }
   ],
   "source": [
    "print('Input folder:', input_folder)\n",
    "print('Project name:', project_name)\n",
    "print('Batch id:', batch_id)\n",
    "print('Channels dictionary:', ch_dic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43510e62",
   "metadata": {},
   "source": [
    "## Load data generator\n",
    "\n",
    "- Run next cell to generate the load data using regex, filenames and folder names. Both files will be saved in the input folder inside a load_data_csv folder.\n",
    "\n",
    "- IMPORTANT: this will run and generate load_data.csv and load_data_with_illum.csv because we have True and False in the illum_list. **load_data.csv only = False**\n",
    "and **load_data_with_illum.csv only = True**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "614f932b",
   "metadata": {},
   "source": [
    "### Choose: Use this CSV locally or on AWS\n",
    "\n",
    "- Important to determine if this output CSV will have the path to a AWS machine or to be used locally (meaning the input path would be also the images_dir variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6590e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws = False #change to False if you're using this CSV LOCALLY "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c504f9a",
   "metadata": {},
   "source": [
    "# add Width of the image\n",
    "\n",
    "# add a variation for representative_cells that removes the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1469960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "illum_list = [True, False]\n",
    "\n",
    "for illum_bool in illum_list:\n",
    "    df = pd.DataFrame()\n",
    "    #plate name\n",
    "    regex_plate = r\".*[\\\\/](?P<Assay>.*)[\\\\/](?P<Plate>.*)$\"\n",
    "    plate_search = re.search(regex_plate, input_folder)\n",
    "    platefind = plate_search.group('Plate')\n",
    "    plate = platefind.replace(\" \", \"_\")\n",
    "    if aws:\n",
    "        images_dir = \"/home/ubuntu/bucket/projects/\" + project_name + \"/\" + batch_id + \"/images/\" + plate + \"/images\"\n",
    "        illum_dir = \"/home/ubuntu/bucket/projects/\" + project_name + \"/\" + batch_id + \"/illum/\" + plate\n",
    "    else:\n",
    "        images_dir = input_folder\n",
    "        illum_dir = input_folder + r\"/illum\"\n",
    "    #filesname and channel\n",
    "    files = []\n",
    "    for (dirpath, dirnames, filenames) in walk(input_folder):\n",
    "        files.extend(sorted(filenames))\n",
    "        break\n",
    "    #find channels\n",
    "    channels = []\n",
    "    regex = r\"^(?P<Well>.*)_.*_.*_(?P<Site>.*)_(?P<Channel>.*)_001.tif\"\n",
    "    for f in files:\n",
    "        matches = re.search(regex, f)\n",
    "        if matches:\n",
    "            channels.append(matches.group('Channel'))\n",
    "    channels = np.array(channels)\n",
    "    ch_unique = np.unique(channels)\n",
    "    #create columns with files and pathnames\n",
    "    temp_list = []\n",
    "    illum_list = []\n",
    "    for ch in ch_unique:\n",
    "        temp_list = []\n",
    "        for file in files:\n",
    "            if ch in file and 'tif' in file:\n",
    "                temp_list.append(file)\n",
    "        if ' ' in ch:\n",
    "            ch = ch.replace(' ', '')\n",
    "        for key,value in ch_dic.items():\n",
    "            if key in ch:\n",
    "                df[\"FileName_\"+value] = temp_list\n",
    "                # print(temp_list)\n",
    "                df[\"PathName_\"+value] = images_dir\n",
    "                if illum_bool:\n",
    "                    illum_temp = plate + \"_Illum\" + value + \".npy\"\n",
    "                    df[\"FileName_Illum\"+value] = illum_temp\n",
    "                    df[\"PathName_Illum\"+value] = illum_dir\n",
    "    #get wells and sites names\n",
    "    wells = []\n",
    "    sites = []\n",
    "    for files in df.iloc[:, 0]:\n",
    "        matches = re.search(regex, files)\n",
    "        wells.append(matches.group('Well'))\n",
    "        sites.append(matches.group('Site'))\n",
    "    df['Metadata_Well'] = wells\n",
    "    df['Metadata_Site'] = sites\n",
    "    df['Metadata_Plate'] = plate\n",
    "    #save df\n",
    "    directory = \"load_data_csv\"\n",
    "    path = os.path.join(input_folder, directory)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    if illum_bool:\n",
    "        df.to_csv(path + r'\\load_data_with_illum.csv', index=False)\n",
    "    else:\n",
    "        df.to_csv(path + r'\\load_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0492f7ce",
   "metadata": {},
   "source": [
    "## AWS commands\n",
    "\n",
    "- Cells below will generate an output that can be copied and paste into Command prompt (after installing AWS cli). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81d979c8",
   "metadata": {},
   "source": [
    "### Click below to copy the command and upload your CSVs into the cloud in the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dac44842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bdafb9453a42179c47df9ac94abf71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(description='Copy', style=ButtonStyle()), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create the command\n",
    "load_csv = \"s3://imaging-platform-ssf/projects/\" + project_name + \"/workspace/load_data_csv/\"+ batch_id + \"/\" + plate + \"/\"\n",
    "load_csv_input = input_folder + \"\\load_data_csv\"\n",
    "load_csv_output = 'aws s3 sync \"' + load_csv_input + '\" \"' + load_csv + '\" --exclude \"*\" --include=\"*.csv\"'\n",
    "#button to copy the command\n",
    "button = widgets.Button(description='Copy')\n",
    "out = widgets.Output()\n",
    "def on_button_clicked(_):\n",
    "      # \"linking function with output\"\n",
    "      with out:\n",
    "        # what happens when we press the button\n",
    "        clear_output()\n",
    "        print(load_csv_output)\n",
    "        pyperclip.copy(load_csv_output)\n",
    "        spam = pyperclip.paste()\n",
    "# linking button and function together using a button's method\n",
    "button.on_click(on_button_clicked)\n",
    "# displaying button and its output together\n",
    "widgets.VBox([button,out])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "678945ea",
   "metadata": {},
   "source": [
    "### Click below to copy the command and upload your IMAGES into the cloud in the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "40e8c1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa24fba13b6541c3a4b92e5a4d98c902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(description='Copy', style=ButtonStyle()), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create the command\n",
    "images = \"s3://imaging-platform-ssf/projects/\"+ project_name + \"/\" + batch_id + \"/images/\" + plate + \"/images/\"\n",
    "images_output = 'aws s3 sync \"' + input_folder + '\" \"' + images + '\" --exclude \"*\" --include=\"*.tif\"'\n",
    "#button to copy the command\n",
    "button = widgets.Button(description='Copy')\n",
    "out = widgets.Output()\n",
    "def on_button_clicked(_):\n",
    "      # \"linking function with output\"\n",
    "      with out:\n",
    "        # what happens when we press the button\n",
    "        clear_output()\n",
    "        print(images_output)\n",
    "        pyperclip.copy(images_output)\n",
    "        spam = pyperclip.paste()\n",
    "# linking button and function together using a button's method\n",
    "button.on_click(on_button_clicked)\n",
    "# displaying button and its output together\n",
    "widgets.VBox([button,out])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89494a41",
   "metadata": {},
   "source": [
    "# Troubleshooting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e32547ec",
   "metadata": {},
   "source": [
    "## Check for disparity in channels numbers\n",
    "\n",
    "- If you encounter some error when generating the CSVs, it could be related to a disparity in the number of images (for some reason, you have one of the Wells with one image more or less in one of the channels. It could be some error on the microscope when saving images, etc.). \n",
    "\n",
    "- Run the code below to print the number of images in each channel. If the channel has more images than other, you'll see a difference in number of images.\n",
    "\n",
    "- Change where indicated in the code for the name of disparity channel (e.g., DAPI) and it will print the name of the wells and the number of images. There, you will find which well has one picture more or less than the others. \n",
    "\n",
    "- From that you can investigate what's wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "037111d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211015_065907_Plate_1\n",
      "160 GFP\n",
      "160 Propidium Iodide\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "wells = []\n",
    "df = pd.DataFrame()\n",
    "#plate name\n",
    "regex_plate = r\".*[\\\\/](?P<Assay>.*)[\\\\/](?P<Plate>.*)$\"\n",
    "plate_search = re.search(regex_plate, input_folder)\n",
    "platefind = plate_search.group('Plate')\n",
    "plate = platefind.replace(\" \", \"_\")\n",
    "print(plate)\n",
    "images_dir = \"/home/ubuntu/bucket/projects/\" + project_name + \"/\" + batch_id + \"/images/\" + plate + \"/images\"\n",
    "illum_dir = \"/home/ubuntu/bucket/projects/\" + project_name + \"/\" + batch_id + \"/illum/\" + plate\n",
    "#filesname and channel\n",
    "files = []\n",
    "for (dirpath, dirnames, filenames) in walk(input_folder):\n",
    "    files.extend(filenames)\n",
    "    break\n",
    "#find channels\n",
    "channels = []\n",
    "regex = r\"^(?P<Well>.*)_.*_.*_(?P<Site>.*)_(?P<Channel>.*)_001.tif\"\n",
    "for f in files:\n",
    "    matches = re.search(regex, f)\n",
    "    if matches:\n",
    "        channels.append(matches.group('Channel'))\n",
    "channels = np.array(channels)\n",
    "ch_unique = np.unique(channels)\n",
    "#create cols with files and pathnames\n",
    "temp_list = []\n",
    "illum_list = []\n",
    "for ch in ch_unique:\n",
    "    temp_list = []\n",
    "    for file in files:\n",
    "        if ch in file and 'tif' in file:\n",
    "            temp_list.append(file)\n",
    "    temp = []\n",
    "    for i in temp_list:\n",
    "        temp.append(i)\n",
    "    print(len(temp), ch)\n",
    "    # CHANGE HERE\n",
    "    if ch == 'DAPI': #CHANGE HERE FOR THE CHANNEL WITH THE DISPARITY NUMBER\n",
    "        for files in temp:\n",
    "            matches = re.search(regex, files)\n",
    "            wells.append(matches.group('Well'))\n",
    "my_dict = {i:wells.count(i) for i in wells}\n",
    "print(my_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd78521a938a9c4193d0781f83c0de96f4f266db216f064a23343888b0dd2e37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
